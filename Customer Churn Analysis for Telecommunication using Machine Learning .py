# -*- coding: utf-8 -*-
"""Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EJiXdq2xg3lAZO-ayLEV7a3OTKRRCj3D

#**`CUSTOMER CHURN ANALYSIS USING MACHINE LEARNING`**

I will be using the Customer Signature for Churn Analysis dataset for this project, and I got this dataset from the enterprise data catalogue, [data.world](https://data.world/bob-wakefield/call-center-data). `Adaryl Bob Wakefield`, a Data Management Expert, provided the dataset. The dataset contains real anonymized telecom customer data from the telecommunication industry.

The marketing research problem I will be researching is `Customer Churn`. `Customer churn` is the process of subscribers switching from one service provider to another which can be voluntary or involuntary.

#**`IMPORTING LIBRARIES`**
"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

"""#**`EXPLORATORY DATA ANALYSIS (EDA)`**"""

df = pd.read_csv("/content/drive/MyDrive/Datasets/customer_data_(Churn Analysis).csv")
df.head()

df.info()
# checking general information about dataframe

df.isnull().sum()
# there are no null values in each of the columns

df.shape
# checking dataframe shape

df.dtypes
# checking dataframe column types

df.describe()
# summary statistics on numeric variable types

print("\nFeatures : \n" ,df.columns)
# printing columns to list

df["churn"].value_counts()
# counting number of customers who did not churn or churned

"""`11069 Customers did not churn
1823 Customers churned`
"""

fig, ax = plt.subplots(1, figsize=(7,7))
sns.countplot(data=df, x='churn')

fig, ax = plt.subplots(1, figsize=(7,7))
sns.boxplot(data=df,
            y='account_length',
            x="churn",
            sym="")

"""I created the plot above to check if churning had an effect on account lengths. I observed that churn did not have an effect on the length of customers accounts. The spread of the data and median value are identical."""

fig, ax = plt.subplots(5, 3, figsize=(10,9))

plt.style.use('ggplot')
#plt.style.use('default')
#print(plt.style.available)

ax[0, 0].hist(df['total_day_minutes'], bins = 35, ec='black', color = "r")

ax[0, 1].hist(df['total_eve_minutes'], bins = 35, ec='black', color = "g")

ax[0, 2].hist(df['account_length'], bins = 35, ec='black', color = "orange")

ax[1, 0].hist(df['total_eve_charge'], bins = 35, ec='black', color = "brown")

ax[1, 1].hist(df['total_night_minutes'], bins = 35, ec='black', color = "b")

ax[1, 2].hist(df['total_intl_minutes'], bins = 35, ec='black',color = "tan")

ax[2, 0].hist(df['number_vmail_messages'], bins = 35, ec='black',color = "blueviolet")

ax[2, 1].hist(df['total_night_calls'], bins = 35, ec='black', color = "brown")

ax[2, 2].hist(df['total_day_calls'], bins = 35, ec='black', color = "teal")

ax[3, 0].hist(df['total_day_charge'], bins = 35, ec='black', color = "lime")

ax[3, 1].hist(df['total_eve_calls'], bins = 35, ec='black', color = "slategray")

ax[3, 2].hist(df['total_night_charge'], bins = 35, ec='black', color = "gold")

ax[4, 0].hist(df['total_intl_calls'], bins = 35, ec='black', color = "navy")

ax[4, 1].hist(df['total_intl_charge'], bins = 35, ec='black', color = "gray")

ax[4, 2].hist(df['number_customer_service_calls'], bins = 35, ec='black', color = "rosybrown")


ax[0, 0].set_xlabel("total_day_minutes")
ax[0, 1].set_xlabel("total_eve_minutes")
ax[0, 2].set_xlabel("account_length")
ax[1, 0].set_xlabel("total_eve_charge")
ax[1, 1].set_xlabel("total_night_minutes")
ax[1, 2].set_xlabel("total_intl_minutes")
ax[2, 0].set_xlabel("number_vmail_messages")
ax[2, 1].set_xlabel("total_night_calls")
ax[2, 2].set_xlabel("total_day_calls")
ax[3, 0].set_xlabel("total_day_charge")
ax[3, 1].set_xlabel("total_eve_calls")
ax[3, 2].set_xlabel("total_night_charge")
ax[4, 0].set_xlabel("total_intl_calls")
ax[4, 1].set_xlabel("total_intl_charge")
ax[4, 2].set_xlabel('number_customer_service_calls')


ax[0, 0].set_ylabel("Count")
ax[0, 1].set_ylabel("Count")
ax[0, 2].set_ylabel("Count")
ax[1, 0].set_ylabel("Count")
ax[1, 1].set_ylabel("Count")
ax[1, 2].set_ylabel("Count")
ax[2, 0].set_ylabel("Count")
ax[2, 1].set_ylabel("Count")
ax[2, 2].set_ylabel("Count")
ax[3, 0].set_ylabel("Count")
ax[3, 1].set_ylabel("Count")
ax[3, 2].set_ylabel("Count")
ax[4, 0].set_ylabel("Count")
ax[4, 1].set_ylabel("Count")
ax[4, 2].set_ylabel('Count')

fig.suptitle('HISTOGRAM DISTRIBUTION', y=1.05)

fig.tight_layout()

plt.show()

"""I made an histogram to check the distribution of the features"""

fig, axes = plt.subplots(1, 3, figsize=(17,7))

sns.boxplot(data=df, x='churn', y='number_customer_service_calls', ax=axes[0], sym="")
sns.boxplot(data=df, x='churn', y='number_customer_service_calls', hue="voice_mail_plan", sym="", ax=axes[1])
sns.boxplot(data=df, x='churn', y='number_customer_service_calls',hue="international_plan",sym="", ax=axes[2])

plt.show()

"""I created the plot above to check if having a voice mail plan or international plan had an effect on the churn rate.I observed that customers who churn left more customer service calls. Furthermore, customers who did not have an international plan or and voice mail plan left more customer service calls compared to those that had an international plan or and voice mail plan.

Having voice mail plans or and international plan did not have an effect on the number of customer service calls for non-churners.

#**`DATA PREPROCESSING`**
"""

df.dtypes

"""**`ENCODING`**

I `encoded` some of my variables because they are none numeric variables.
"""

#i will be encoding the variables listed below

print(df["international_plan"].unique())
print(df["voice_mail_plan"].unique())
print(df["churn"].unique())

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

df["international_plan"] = le.fit_transform(df["international_plan"])
df["voice_mail_plan"] = le.fit_transform(df["voice_mail_plan"])
df["churn"] = le.fit_transform(df["churn"])

print(df["international_plan"].unique())
print(df["voice_mail_plan"].unique())
print(df["churn"].unique())

df.dtypes

"""`DROPPING COLUMNS`"""

df = df.drop(["recordid","customer_id", "state", "area_code"], axis = 1)

df.columns.tolist()

"""Unique identifiers such as `recordid`, `area_code`, and `customer_id` do not have predicitve power thus they are not pertinent to the analysis.

`CHECKING FOR CORRELATION`
"""

corr = df.corr()
corr.style.background_gradient(cmap='coolwarm')

"""After looking at the correlation map the following features are highly correlated hence one of each have to be dropped this is beacause highly correlated features do not provide additional information, but may increase the complexity of the model, thus increasing the risk of error.

1. number_vmail_messages & voice_mail_plan
2. total_day_charge & total_day_minutes
3. total_eve_charge & total_eve_minutes
4. total_night_charge & total_night_minutes
5. total_int_charge & total_int_minutes
"""

df = df.drop(["number_vmail_messages","total_day_minutes", "total_eve_minutes", "total_night_charge", "total_intl_charge"], axis = 1)

df.columns.tolist()

df.dtypes

df

"""#`USING PYCARET`

PyCaret is an open-source, low-code machine learning library in Python that automates machine learning workflows. It is an end-to-end machine learning and model management tool that exponentially speeds up the experiment cycle and makes you more productive.

[REFRENCE](https://www.datacamp.com/tutorial/guide-for-automating-ml-workflows-using-pycaret/)
"""

#!pip install pycaret

from pycaret.utils import enable_colab
enable_colab()

from pycaret.regression import *
s = setup(data = df, target = 'churn')

best = compare_models()

"""`I used Pycaret above to get an idea of the types of models which could be applied to solve the problem.`

#`MODEL BUILDING`

`SPLITTING DATASET INTO TRAINING AND TESTING`
"""

X = df.drop('churn', axis = 1) #predictor variable(s)
y = df["churn"] #target varaible

X.head()

y.head()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)  #splitting the raw dataset in X_train, X_test, y_train, y_test

print(f"Shape of X_train after splitting = {X_train.shape}", '\n')
print(f"Shape of X_test after splitting = {X_test.shape}", '\n')
print(f"Shape of y_train after splitting = {y_train.shape}", '\n')
print(f"Shape of y_test after splitting = {y_test.shape}", '\n')

"""**`FEATURE SCALING`**

I performed `Feature scaling` to normalize the range of the features because they have varying degree of magnitude.
"""

from sklearn.preprocessing import StandardScaler
Ss = StandardScaler()

X_train = Ss.fit_transform(X_train)
X_test = Ss.transform(X_test)

#sklearn standardscaler works by taking the mean of the distribution and then finds the standard deviation for each data point from the mean

X_train

X_test

"""**`MODEL TRAINING FOR DIFFERENT ALGORITHMS`**"""

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)
print(rfc.score(X_test, y_test))

from sklearn.tree import DecisionTreeClassifier
dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
print(dtc.score(X_test, y_test))

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train, y_train)
print(lr.score(X_test, y_test))

"""**`MODEL EVALUATION (CONFUSION MATRIX)`**"""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
import warnings
warnings.filterwarnings("ignore")

y_pred_1 = rfc.predict(X_test)

print('RANDOM FOREST CLASIFIER')
print(confusion_matrix(y_test, y_pred_1), '\n')

print('Precision: %.3f' % precision_score(y_test, y_pred_1), '\n')
print('Accuracy: %.3f' % accuracy_score(y_test, y_pred_1), '\n')
print('Recall: %.3f' % recall_score(y_test, y_pred_1), '\n')
print('f1_score: %.3f' % f1_score(y_test, y_pred_1), '\n')

plot_confusion_matrix(rfc, X_test, y_test)
plt.show()

y_pred_2 = dtc.predict(X_test)
print('DECISION TREE CLASIFIER')
print(confusion_matrix(y_test, y_pred_2), '\n')

print('Precision: %.3f' % precision_score(y_test, y_pred_2), '\n')
print('Accuracy: %.3f' % accuracy_score(y_test, y_pred_2), '\n')
print('Recall: %.3f' % recall_score(y_test, y_pred_2), '\n')
print('f1_score: %.3f' % f1_score(y_test, y_pred_2), '\n')

plot_confusion_matrix(dtc, X_test, y_test)
plt.show()

y_pred_3 = lr.predict(X_test)
print('LOGISITC REGRESSION')
print(confusion_matrix(y_test, y_pred_3), '\n')

print('Precision: %.3f' % precision_score(y_test, y_pred_3), '\n')
print('Accuracy: %.3f' % accuracy_score(y_test, y_pred_3), '\n')
print('Recall: %.3f' % recall_score(y_test, y_pred_3), '\n')
print('f1_score: %.3f' % f1_score(y_test, y_pred_3), '\n')

plot_confusion_matrix(lr, X_test, y_test)
plt.show()

"""#`AGGREGATING MODEL RESULTS`"""

from tabulate import tabulate

stayed = [['MACHINE LEARNING ALGORITHMS', 'PRECISION', 'ACCURACY', "RECALL", "F1_SCORE"] ,
          ['RANDOM FOREST CLASIFIER', 0.975, 0.988, 0.946, 0.960],
          ['DECISION TREE CLASIFIER', 0.964, 0.986, 0.942, 0.953],
          ['LOGISITC REGRESSION',     0.672, 0.868, 0.209, 0.319]]

print(tabulate(stayed,headers='firstrow', tablefmt='fancy_grid')) #Stayed

"""From the table above we can conclude that the `RANDOM FOREST CLASSIFER` model is the best model compared to the other two given it's high-level `PRECISION`, `ACCURACY`, `RECALL` and `F1-SCORE`.

#`RECOMMENDATIONS`

I created a new feature total minute, by combining all call minutes made during the day, evening, and night. I observed that customers having voicemail plans made more calls than those who did not. Given this, I would recommend cell providers incentivize their customers to get voicemail plans since increased call times translate to customers spending more money thus increasing revenue.

Furthermore, customers who did not have an international plan spent more time on the phone than those who had. This suggests that having an international plan is not a major driver for customer engagement. I would recommend that service providers divert their resources to other value-added benefits.
"""
